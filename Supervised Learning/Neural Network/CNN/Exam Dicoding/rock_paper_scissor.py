# -*- coding: utf-8 -*-
"""Rock Paper Scissor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cz5gUB9D1xHnfPVdC3lSQQ9pDUVnsM14
"""

import pandas as pd
import numpy as np
import os
import cv2
import tensorflow as tf
import sys
import pathlib
import matplotlib.pyplot as plt
import numpy

"""Hal yang perlu di ingat ketika ingin melakukan analisis Deep Learning adalah data preparation

1. Alat yang digunakan bisa menggunakan jupyter notebook maupun google colab. Namun sebaiknya menggunakan google colab untuk menghindari adanya crash dependency 

2. Upload data bisa menggunakan local computer, upload data ke google drive maupun menggunakan url. Jika cara menggunakan url gagal atau susah bisa menggunakan cara upload data dulu ke google drive.

3. Sebelum di upload ke google drive, data terlebih dahulu harus dibagi ke dua folder train dan test. 

4. Untuk mendapatkan path dari data yang sudah diupload, bisa menggunakan seperti cara mencari data di mac atau ubuntu. Yaitu %pwd untuk mengetahui lokasi saat ini, %ls untuk menampilkan list file dan %cd untuk berpindah direktori

5. Cek dan pastikan data telah berhasil di input

6. Buatlah 1 list yang berisi data training. Data yang awalnya berbentuk image perlu diubah ke dalam bentuk matriks. Warna image perlu diubah ke dalam bentuk warna grayscale agar tidak memberatkan komputasi.Image yang mempunyai warna akan membuat ukuran matriks akan menjadi besar. Padahal sebenarnya warna tidak akan mempengaruhi analisis secara siginigikan. Sehingga jika diubah ke grayscale pun tidak menjadi masalah

7. Image juga bisa diubah ke dimensinya ke yang lebih kecil, efeknya gambar tidak terlalu tajam, namun sebisa mungkin masih bisa menggambarkan sebagian besar makna gambar tersebut.

8. Pada data training, perlu dilakukan pengacakan urutan agar satu kelas tidak mengumpul.

9.
"""

from google.colab import drive
drive.mount('/content/gdrive')

!ls "content/gdrive/My Drive/DATASET"

# Commented out IPython magic to ensure Python compatibility.
# %pwd

# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %cd 'gdrive'

# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %cd 'My Drive'

# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# % cd 'DATASET'

# Commented out IPython magic to ensure Python compatibility.
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %cd 'Rock-Paper-Scissors'

# Commented out IPython magic to ensure Python compatibility.
# %pwd

!ls '/content/gdrive/My Drive/DATASET/Rock-Paper-Scissors'

data_dir = '/content/gdrive/My Drive/DATASET/Rock-Paper-Scissors/Train'

categories = ['paper','rock','scissors']

for k in categories:
	path = os.path.join(data_dir,k)
	for img in os.listdir(path):
		img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)
		plt.imshow(img_array,cmap='gray')
		plt.show 
		break
	break

print(img_array)

print(img_array.shape)

img_size = 300

new_array = cv2.resize(img_array,(img_size,img_size))
plt.imshow(new_array,cmap='gray')
plt.show()

training_data = []
def create_training_data():
	for k in categories:
		path = os.path.join(data_dir,k)
		class_num = categories.index(k)
		for img in os.listdir(path):
			try:
				img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)
				new_array = cv2.resize(img_array,(img_size,img_size))
				training_data.append([new_array,class_num])
			except Exception as e:
				pass
        
create_training_data()

print(len(training_data))

for sample in training_data[:10]:
  print(sample[1])

import random

random.shuffle(training_data)

for sample in training_data[:10]:
  print(sample[1])

X = []
y = []
for features, label in training_data:
  X.append(features)
  y.append(label)

X = np.array(X).reshape(-1,img_size,img_size,1)
y = np.array(y)

import pickle

pickle_out = open("X.pickle","wb")
pickle.dump(X,pickle_out)
pickle_out.close()

pickle_out = open("y.pickle","wb")
pickle.dump(y,pickle_out)
pickle_out.close()

pickle_in = open("X.pickle","rb")
X = pickle.load(pickle_in)

pickle_in = open("y.pickle","rb")
y = pickle.load(pickle_in)

X[1]

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D
import pickle

X = pickle.load(open("X.pickle","rb"))
y = pickle.load(open("y.pickle","rb"))
X = X/225

type(X)

type(y)

model = Sequential()

model.add(Conv2D(64,(3,3),input_shape=X.shape[1:]))
model.add(Activation("relu"))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3)))
model.add(Activation("relu"))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Flatten())
model.add(Dense(64))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss="binary_crossentropy",
              optimizer="adam",
              metrics=['accuracy'])

model.fit(X,y,batch_size = 32,epochs=3,validation_split = 0.1)

"""## REFFERENCE
1. https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/images.ipynb#scrollTo=rN-Pc6Zd6awg

2. https://support.google.com/drive/thread/13694646?hl=en

3. https://pythonprogramming.net/convolutional-neural-network-deep-learning-python-tensorflow-keras/

4. https://www.youtube.com/watch?v=s-V7gKrsels
"""

